```python
# pip install transformers==4.21.0 datasets evaluate
# pip install nltk rouge-score

import nltk
nltk.download("punkt")
```

这次文本摘要任务使用的数据集是ACL文论的摘要标题数据集，包含来自ACL的10874个摘要和标题对，任务即输入摘要输出标题（也可以看做是文本摘要，对吧^_^），下载地址如下：[EagleW/ACL_titles_abstracts_dataset (github.com)](https://github.com/EagleW/ACL_titles_abstracts_dataset)

数据格式如下图所示，标题和摘要各占一行，数据之间用空行分割。

![img](https://pic2.zhimg.com/80/v2-6b51d2fde0e2bf8a74c02fbef50d397d_720w.webp)

针对这种格式的数据，我们需要自行写加载脚本，命名为load_script.py，代码如下：

```python
import datasets
from datasets import DownloadManager, DatasetInfo

logger = datasets.logging.get_logger(__name__)

_DOCUMENT = "abstract"
_SUMMARY = "title"


class AclSummarization(datasets.GeneratorBasedBuilder):

    def _info(self) -> DatasetInfo:
        """
            info方法，要定义数据集的信息
            *** 定义 feature
            涉及两个字段：_DOCUMENT和_SUMMARY，datasets.Value()声明字段的类型
        :return:
        """
        return datasets.DatasetInfo(
            description="ACL标题摘要数据集",
            features=datasets.Features({_DOCUMENT: datasets.Value("string"), _SUMMARY: datasets.Value("string")}),
        )

    def _split_generators(self, dl_manager: DownloadManager):
        """
            返回datasets.SplitGenerator
            涉及两个参数：name和gen_kwargs
            name: 指定数据集的划分
            gen_kwargs: 指定要读取的文件的路径，与_generate_examples的入参数一致
        :param dl_manager:
        :return: [ datasets.SplitGenerator ]
        """
        return [datasets.SplitGenerator(name=datasets.Split.TRAIN,
                                        gen_kwargs={"filepath": "./acl_titles_and_abstracts.txt"})]

    def _generate_examples(self, filepath):
        """
            生成具体的样本，使用yield
            需要额外指定key，id从0开始自增就可以
        :param filepath:
        :return:
        """
        logger.info("generating examples from = %s", filepath)
        key = 0
        with open(filepath, encoding="utf-8") as f:
            example = {}
            for line in f.readlines():
                if line.strip() == "":
                    yield key, example
                    example = {}
                    key += 1
                else:
                    if _SUMMARY not in example:
                        example[_SUMMARY] = line.strip()
                    else:
                        example[_DOCUMENT] = line.strip()
```

核心代码：

```python
import nltk
import datasets
import evaluate
import numpy as np
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq

# 加载数据集
dataset = datasets.load_dataset("./load_script.py", split="train")
datasets = dataset.train_test_split(test_size=0.2)

# 数据集处理
tokenizer = AutoTokenizer.from_pretrained("google/t5-efficient-tiny")
prefix = "summarize: "
def preprocess_function(examples):
    inputs = [prefix + doc for doc in examples["abstract"]]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True)
    labels = tokenizer(examples["title"], max_length=128, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
tokenized_dataset = datasets.map(preprocess_function, batched=True)

# 构建评估函数
rouge_metric = evaluate.load("rouge")
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds = ["\n".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]
    
    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
    result = {key: value * 100 for key, value in result.items()}
 
    return {k: round(v, 4) for k, v in result.items()}

# 配置训练器
model = AutoModelForSeq2SeqLM.from_pretrained("google/t5-efficient-tiny")
training_args = Seq2SeqTrainingArguments(
    learning_rate=3e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=32,
    num_train_epochs=5,
    weight_decay=0.01,
    output_dir="model_for_seq2seqlm",
    logging_steps=10,
    evaluation_strategy = "epoch",
    save_strategy = "epoch",
    load_best_model_at_end=True,
    metric_for_best_model="rougeL",
    predict_with_generate=True   # 训练最后会调用generate方法进行生成
)
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),
)

# 训练与评估
trainer.train()

trainer.evaluate(tokenized_dataset["test"], num_beams=3)
```

进行预测：

```python
inputs = tokenizer("summarize: " + datasets["test"][0]["abstract"], return_tensors="pt", max_length=512, truncation=True)
inputs = {k:v.cuda() for k, v in inputs.items()}
outputs = model.generate(inputs["input_ids"], num_beams=3)
ex_id = 0
print("REAL TITLE:", datasets["test"][ex_id]["title"])
print("=" * 30)
print("PRED TITLE:", tokenizer.decode(outputs[0], skip_special_tokens=True))
```

或者使用pipeline进行预测：

```python
from transformers import pipeline

model = AutoModelForSeq2SeqLM.from_pretrained("model_for_seq2seqlm/checkpoint-5440")
tokenizer = AutoTokenizer.from_pretrained("model_for_seq2seqlm/checkpoint-5440")
summarizer = pipeline("summarization", model=model, tokenizer=tokenizer)
text = "learning taxonomy for technical terms is difficult and tedious task , especially when new terms should be included . the goal of this paper is to assign taxonomic relations among technical terms . we propose new approach to the problem that relies on term specificity and similarity measures . term specificity and similarity are necessary conditions for taxonomy learning , because highly specific terms tend to locate in deep levels and semantically similar terms are close to each other in taxonomy . we analyzed various features used in previous researches in view of term specificity and similarity , and applied optimal features for term specificity and similarity to our method ." 
prefix = "summarize: "
text = prefix + text
summarizer(text)
```



