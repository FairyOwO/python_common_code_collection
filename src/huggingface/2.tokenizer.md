## 加载分词器

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
```

## 句子分词

```python
sen = "弱小的我也有大梦想"
tokens = tokenizer.tokenize(sen)
```

## 查看vocab

```python
tokenizer.vocab
```

## 转换

```python
ids = tokenizer.convert_tokens_to_ids(tokens)
tokens = tokenizer.convert_ids_to_tokens(ids)
```

## 编码

```python
encode_dict = tokenizer.encode(sen, 
                       padding="max_length", 
                       max_length=15, 
                       truncation=True, 
                       return_attention_mask=True,
                       return_token_type_ids=True)
```

或者

```python
tokens_a = '我 爱 北 京 天 安 门'.split(' ')
tokens_b = '我 爱 打 英 雄 联 盟 啊 啊'.split(' ')

encode_dict = tokenizer.encode_plus(text=tokens_a,
                  text_pair=tokens_b,
                  max_length=20,
                  pad_to_max_length=True,
                  truncation_strategy='only_second',
                  is_pretokenized=True,
                  return_token_type_ids=True,
                  return_attention_mask=True)
```

当然还有batch_encode()和batch_encode_plus()进行批量处理的。需要注意的是不同版本的transformers可能里面的参数有稍许不同。

## 补充

当然我们也可以直接使用tokenizer来对文本进行处理、会返回attention_mask和token_type_ids。

```python
text = ['我爱北京', '我爱北京天安门']
raw_tokens = tokenizer(['我爱北京', '我爱北京天安门'])
```

