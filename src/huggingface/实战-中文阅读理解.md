```python
!pip install transformers==4.21.0 datasets evaluate nltk
```

## 长文本问题

一般来说，预训练模型能接受的输入长度都是有限制的，以bert模型为例，最大长度就是512，往往一篇文章的长度不止于此，因此需要做特殊的处理，使用滑动窗口的方式解决该问题，即每次问题与窗口大小的文章进行拼接组成一个新的输入，而后根据给定步长进行滑动，直至滑过整篇文章。

tokenizer中提供了该方式的快捷调用方式，只需要指定`return_overflowing_tokens`参数和`stride`参数即可，前者表示要返回滑动窗口的结果，后者指定了重叠部分的长度，示例如下：

```python
tokenized_example= tokenizer(datasets["train"]["question"][:1],
                             datasets["train"]["context"][:1],
                             max_length=384,
                             truncation="only_second",   
                             return_overflowing_tokens=True,
                             stride=128,
                             padding="max_length")

for ipt in tokenized_example["input_ids"]:
  print(tokenizer.decode(ipt))
```

这一伴生问题transformers自然也是考虑到了，在指定`return_overflowing_tokens`后，分词结果中会多一个字段`overflow_to_sample_mapping`，该字段中存储着输入片段与样本的对应关系，

```python
tokenized_example= tokenizer(datasets["train"]["question"][:5],
                             datasets["train"]["context"][:5],
                             max_length=384,
                             truncation="only_second",   
                             return_overflowing_tokens=True,
                             stride=128,
                             padding="max_length")
tokenized_example["overflow_to_sample_mapping"]
```

## 答案对齐问题

其实这一问题在序列标注任务中曾提到过，对于纯中文的数据，使用tokenizer进行切分自然没问题，但数据中会存在英文单词、空格等内容，如果还是直接使用tokenizer进行数据处理，单词可能会被切分为子词，此时第i个token不再代表句子中的第i个字，此时便会涉及到一个答案对齐的问题，即我们的答案到底在分词后token中的哪个位置。

这一问题，tokenizer也提供了解决的方案，需要使用`return_offsets_mapping`参数，指定该参数值为`True`后，分词的结果便会额外返回一个`offset_mapping`字段，该字段中存储着input_ids中每个token对应到原始文本中的位置信息。

```python
tokenized_example = tokenizer(datasets["train"]["question"][:1],
         datasets["train"]["context"][:1],
         max_length=384,
         truncation="only_second",   
         return_overflowing_tokens=True,
         return_offsets_mapping=True,
         stride=128,
         padding="max_length")
print(tokenizer.decode(tokenized_example["input_ids"][0]))
print(tokenized_example["input_ids"][0])
print(tokenized_example["offset_mapping"][0])
```

借助`offset_mapping`信息，便可以根据答案的起始位置进行答案的对齐了，自然就解决了这一问题。

根据前面介绍的这两个问题的解决方案，我们便可以定义数据处理函数了，代码如下：

```python
max_length = 384 
doc_stride = 128
def process_function_for_train(examples):
  examples["question"] = [q.strip() for q in examples["question"]]
  tokenized_examples = tokenizer(
    examples["question"],
    examples["context"],
    max_length=max_length,
    truncation="only_second",   # 指定改参数，将只在第二部分输入上进行截断，即文章部分进行截断
    return_overflowing_tokens=True, # 指定该参数，会根据最大长度与步长将恩本划分为多个段落
    return_offsets_mapping=True,  # 指定改参数，返回切分后的token在文章中的位置
    stride=doc_stride,
    padding="max_length"
  )
  # 对于阅读理解任务，标签数据不再是labels, 而是start_positions和end_positions，分别存储起始和结束位置
  tokenized_examples["start_positions"] = []
  tokenized_examples["end_positions"] = []
  tokenized_examples["example_id"] = []
  # sample_mapping中存储着新的片段对应的原始example的id，例如[0, 0, 0, 1, 1, 2]，表示前三个片段都是第1个example
  # 根据sample_mapping中的映射信息，可以有效的定位答案
  sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
  for i, _ in enumerate(sample_mapping):
    input_ids = tokenized_examples["input_ids"][i]
    answers = examples["answers"][sample_mapping[i]]  # 根据sample_mapping的结果，获取答案的内容
    start_char = answers["answer_start"][0]
    end_char = start_char + len(answers["text"][0])
    sequence_ids = tokenized_examples.sequence_ids(i)

    # 定位文章的起始token位置
    token_start_index = 0
    while sequence_ids[token_start_index] != 1:
      token_start_index += 1

    # 定位文章的结束token位置
    token_end_index = len(input_ids) - 1
    while sequence_ids[token_end_index] != 1:
      token_end_index -= 1

    offsets = tokenized_examples["offset_mapping"][i]

    # 判断答案是否在当前的片段里，条件：文章起始token在原文中的位置要小于答案的起始位置，结束token在原文中的位置要大于答案的结束位置
    # 如果不满足，则将起始与结束位置均置为0
    if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):
      tokenized_examples["start_positions"].append(0)
      tokenized_examples["end_positions"].append(0)
    else: # 如果满足，则将答案定位到token的位置上
      while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
        token_start_index += 1
      tokenized_examples["start_positions"].append(token_start_index - 1)
      while offsets[token_end_index][1] >= end_char:
        token_end_index -= 1
      tokenized_examples["end_positions"].append(token_end_index + 1)

    # 定位答案相关
    tokenized_examples["example_id"].append(examples["id"][sample_mapping[i]])
    tokenized_examples["offset_mapping"][i] = [
        (o if sequence_ids[k] == 1 else None)
        for k, o in enumerate(tokenized_examples["offset_mapping"][i])
    ]
  return tokenized_examples
```

代码中关键部分都做了注释，需要额外注意的是，在阅读理解任务中，标签不再是存储在`labels`字段，而是`start_positions`和`end_positions`，分别存储答案的起始与结束位置。

定义完数据处理函数，便可以使用map方法，对数据集进行处理，不要忘了指定batched参数值为True，这样会加速数据处理。**此外，特别需要的注意的是，需要指定`remove_columns`参数，因为数据处理方法输入的数据个数和输出的数据个数不一样了，如果不删除原有的columns，会报错**。

```python
tokenized_train_dataset = datasets["train"].map(process_function_for_train, batched=True, remove_columns=datasets["train"].column_names)
tokenized_valid_dataset = datasets["validation"].map(process_function_for_train, batched=True, remove_columns=datasets["validation"].column_names)
tokenized_test_dataset = datasets["test"].map(process_function_for_train, batched=True, remove_columns=datasets["test"].column_names)
tokenized_train_dataset, tokenized_valid_dataset, tokenized_test_dataset
```

## 构建评估函数

评估函数部分我们需要做两部分工作，一部分是实现预测模块，一部分是是实现评估模块。

评估模块较为容易，直接使用了cmrc2018的官方评测脚本，但是原脚本需要输入文件，我们需要对其进行改造。

```python
import sys
import os
import re

import nltk
tokenize = lambda x: nltk.word_tokenize(x)

# split Chinese with English
def mixed_segmentation(in_str, rm_punc=False):
    in_str = str(in_str).lower().strip()
    segs_out = []
    temp_str = ""
    sp_char = ['-',':','_','*','^','/','\\','~','`','+','=',
               '，','。','：','？','！','“','”','；','’','《','》','……','·','、',
               '「','」','（','）','－','～','『','』']
    for char in in_str:
        if rm_punc and char in sp_char:
            continue
        if re.search(r'[\u4e00-\u9fa5]', char) or char in sp_char:
            if temp_str != "":
                ss = nltk.word_tokenize(temp_str)
                segs_out.extend(ss)
                temp_str = ""
            segs_out.append(char)
        else:
            temp_str += char

    #handling last part
    if temp_str != "":
        ss = nltk.word_tokenize(temp_str)
        segs_out.extend(ss)

    return segs_out

# remove punctuation
def remove_punctuation(in_str):
    in_str = str(in_str).lower().strip()
    sp_char = ['-',':','_','*','^','/','\\','~','`','+','=',
               '，','。','：','？','！','“','”','；','’','《','》','……','·','、',
               '「','」','（','）','－','～','『','』']
    out_segs = []
    for char in in_str:
        if char in sp_char:
            continue
        else:
            out_segs.append(char)
    return ''.join(out_segs)

# find longest common string
def find_lcs(s1, s2):
    m = [[0 for i in range(len(s2)+1)] for j in range(len(s1)+1)]
    mmax = 0
    p = 0
    for i in range(len(s1)):
        for j in range(len(s2)):
            if s1[i] == s2[j]:
                m[i+1][j+1] = m[i][j]+1
                if m[i+1][j+1] > mmax:
                    mmax=m[i+1][j+1]
                    p=i+1
    return s1[p-mmax:p], mmax

def calc_f1_score(answers, prediction):
    f1_scores = []
    for ans in answers:
        ans_segs = mixed_segmentation(ans, rm_punc=True)
        prediction_segs = mixed_segmentation(prediction, rm_punc=True)
        lcs, lcs_len = find_lcs(ans_segs, prediction_segs)
        if lcs_len == 0:
            f1_scores.append(0)
            continue
        precision     = 1.0*lcs_len/len(prediction_segs)
        recall         = 1.0*lcs_len/len(ans_segs)
        f1             = (2*precision*recall)/(precision+recall)
        f1_scores.append(f1)
    return max(f1_scores)

def calc_em_score(answers, prediction):
    em = 0
    for ans in answers:
        ans_ = remove_punctuation(ans)
        prediction_ = remove_punctuation(prediction)
        if ans_ == prediction_:
            em = 1
            break
    return em

# predictions: {example_id: prediction_text}
# references:  {example_id: [answer1, answer2, ...]}
def evaluate_cmrc(predictions, references):
    f1 = 0
    em = 0
    total_count = 0
    skip_count = 0
    for query_id, answers in references.items():
        total_count += 1
        if query_id not in predictions:
            sys.stderr.write('Unanswered question: {}\n'.format(query_id))
            skip_count += 1
            continue
        prediction = predictions[query_id]
        f1 += calc_f1_score(answers, prediction)
        em += calc_em_score(answers, prediction)
    f1_score = 100.0 * f1 / total_count
    em_score = 100.0 * em / total_count
    return {
        'avg': (em_score + f1_score) * 0.5, 
        'f1': f1_score, 
        'em': em_score, 
        'total': total_count, 
        'skip': skip_count
    }
```

调用evaluate_cmrc函数即可获得分数信息，同样的，我们也需要对齐再进行一层封装，其中get_result函数为获取预测结果的模块。

```python
def compute_metrics(p):
  start_logits, end_logits = p[0]
  if start_logits.shape[0] == len(tokenized_valid_dataset):
    predicted_answers, reference_answers = get_result(start_logits, end_logits, datasets["validation"], tokenized_valid_dataset)
  else:
    predicted_answers, reference_answers = get_result(start_logits, end_logits, datasets["test"], tokenized_test_dataset)
  return evaluate_cmrc(predicted_answers, reference_answers)
```

预测时，我们首先需要进行映射的构建，而后对一个样本的所有预测结果进行汇总，最后取出最优结果，格式与评估函数所需的格式一致即可，代码如下：

```python
def get_result(start_logits, end_logits, examples, features):

  predicted_answers = {}
  reference_answers = {}

  # 构建example到feature的映射
  example_to_features = collections.defaultdict(list)
  for idx, feature_id in enumerate(features["example_id"]):
    example_to_features[feature_id].append(idx)

  # 指定备选最优答案个数与最大答案长度
  n_best = 20
  max_answer_length = 30

  # 抽取答案
  for example in examples:
    example_id = example["id"]
    context = example["context"]
    answers = []
    # 对当前example对应的所有feature片段进行答案抽取
    for feature_index in example_to_features[example_id]:
      start_logit = start_logits[feature_index]
      end_logit = end_logits[feature_index]
      offsets = features[feature_index]["offset_mapping"]
      start_indexes = np.argsort(start_logit)[:: -1][:n_best].tolist()
      end_indexes = np.argsort(end_logit)[:: -1][:n_best].tolist()
      for start_index in start_indexes:
        for end_index in end_indexes:
          if offsets[start_index] is None or offsets[end_index] is None:
            continue
          if (end_index < start_index or end_index - start_index + 1 > max_answer_length):
            continue
          answers.append(
            {
              "text": context[offsets[start_index][0] : offsets[end_index][1]],
              "logit_score": start_logit[start_index] + end_logit[end_index],
            }
          )
    if len(answers) > 0:
      best_answer = max(answers, key=lambda x: x["logit_score"])
      predicted_answers[example_id] = best_answer["text"]
    else:
      predicted_answers[example_id] = ""
    reference_answers[example_id] = example["answers"]["text"]

  return predicted_answers, reference_answers
```

## 所有代码

```python
import evaluate
import collections
import numpy as np
from datasets import load_dataset
from transformers import AutoModelForQuestionAnswering, AutoTokenizer, default_data_collator, TrainingArguments, Trainer

# 加载数据集
datasets = load_dataset("cmrc2018")

tokenizer = AutoTokenizer.from_pretrained('hfl/rbt3')
model = AutoModelForQuestionAnswering.from_pretrained("hfl/rbt3")


# 数据集处理
max_length = 384 
doc_stride = 128
def process_function_for_train(examples):
  examples["question"] = [q.strip() for q in examples["question"]]
  tokenized_examples = tokenizer(
    examples["question"],
    examples["context"],
    max_length=max_length,
    truncation="only_second",   # 指定改参数，将只在第二部分输入上进行截断，即文章部分进行截断
    return_overflowing_tokens=True, # 指定该参数，会根据最大长度与步长将恩本划分为多个段落
    return_offsets_mapping=True,  # 指定改参数，返回切分后的token在文章中的位置
    stride=doc_stride,
    padding="max_length"
  )
  # 对于阅读理解任务，标签数据不再是labels, 而是start_positions和end_positions，分别存储起始和结束位置
  tokenized_examples["start_positions"] = []
  tokenized_examples["end_positions"] = []
  tokenized_examples["example_id"] = []
  # sample_mapping中存储着新的片段对应的原始example的id，例如[0, 0, 0, 1, 1, 2]，表示前三个片段都是第1个example
  # 根据sample_mapping中的映射信息，可以有效的定位答案
  sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
  for i, _ in enumerate(sample_mapping):
    input_ids = tokenized_examples["input_ids"][i]
    answers = examples["answers"][sample_mapping[i]]  # 根据sample_mapping的结果，获取答案的内容
    start_char = answers["answer_start"][0]
    end_char = start_char + len(answers["text"][0])
    sequence_ids = tokenized_examples.sequence_ids(i)

    # 定位文章的起始token位置
    token_start_index = 0
    while sequence_ids[token_start_index] != 1:
      token_start_index += 1

    # 定位文章的结束token位置
    token_end_index = len(input_ids) - 1
    while sequence_ids[token_end_index] != 1:
      token_end_index -= 1

    offsets = tokenized_examples["offset_mapping"][i]

    # 判断答案是否在当前的片段里，条件：文章起始token在原文中的位置要小于答案的起始位置，结束token在原文中的位置要大于答案的结束位置
    # 如果不满足，则将起始与结束位置均置为0
    if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):
      tokenized_examples["start_positions"].append(0)
      tokenized_examples["end_positions"].append(0)
    else: # 如果满足，则将答案定位到token的位置上
      while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
        token_start_index += 1
      tokenized_examples["start_positions"].append(token_start_index - 1)
      while offsets[token_end_index][1] >= end_char:
        token_end_index -= 1
      tokenized_examples["end_positions"].append(token_end_index + 1)

    # 定位答案相关
    tokenized_examples["example_id"].append(examples["id"][sample_mapping[i]])
    tokenized_examples["offset_mapping"][i] = [
        (o if sequence_ids[k] == 1 else None)
        for k, o in enumerate(tokenized_examples["offset_mapping"][i])
    ]
  return tokenized_examples

tokenized_train_dataset = datasets["train"].map(process_function_for_train, batched=True, remove_columns=datasets["train"].column_names)
tokenized_valid_dataset = datasets["validation"].map(process_function_for_train, batched=True, remove_columns=datasets["validation"].column_names)
tokenized_test_dataset = datasets["test"].map(process_function_for_train, batched=True, remove_columns=datasets["test"].column_names)


# 构建评估函数
import sys
import os
import re

import nltk
tokenize = lambda x: nltk.word_tokenize(x)

# split Chinese with English
def mixed_segmentation(in_str, rm_punc=False):
    in_str = str(in_str).lower().strip()
    segs_out = []
    temp_str = ""
    sp_char = ['-',':','_','*','^','/','\\','~','`','+','=',
               '，','。','：','？','！','“','”','；','’','《','》','……','·','、',
               '「','」','（','）','－','～','『','』']
    for char in in_str:
        if rm_punc and char in sp_char:
            continue
        if re.search(r'[\u4e00-\u9fa5]', char) or char in sp_char:
            if temp_str != "":
                ss = nltk.word_tokenize(temp_str)
                segs_out.extend(ss)
                temp_str = ""
            segs_out.append(char)
        else:
            temp_str += char

    #handling last part
    if temp_str != "":
        ss = nltk.word_tokenize(temp_str)
        segs_out.extend(ss)

    return segs_out

# remove punctuation
def remove_punctuation(in_str):
    in_str = str(in_str).lower().strip()
    sp_char = ['-',':','_','*','^','/','\\','~','`','+','=',
               '，','。','：','？','！','“','”','；','’','《','》','……','·','、',
               '「','」','（','）','－','～','『','』']
    out_segs = []
    for char in in_str:
        if char in sp_char:
            continue
        else:
            out_segs.append(char)
    return ''.join(out_segs)

# find longest common string
def find_lcs(s1, s2):
    m = [[0 for i in range(len(s2)+1)] for j in range(len(s1)+1)]
    mmax = 0
    p = 0
    for i in range(len(s1)):
        for j in range(len(s2)):
            if s1[i] == s2[j]:
                m[i+1][j+1] = m[i][j]+1
                if m[i+1][j+1] > mmax:
                    mmax=m[i+1][j+1]
                    p=i+1
    return s1[p-mmax:p], mmax

def calc_f1_score(answers, prediction):
    f1_scores = []
    for ans in answers:
        ans_segs = mixed_segmentation(ans, rm_punc=True)
        prediction_segs = mixed_segmentation(prediction, rm_punc=True)
        lcs, lcs_len = find_lcs(ans_segs, prediction_segs)
        if lcs_len == 0:
            f1_scores.append(0)
            continue
        precision     = 1.0*lcs_len/len(prediction_segs)
        recall         = 1.0*lcs_len/len(ans_segs)
        f1             = (2*precision*recall)/(precision+recall)
        f1_scores.append(f1)
    return max(f1_scores)

def calc_em_score(answers, prediction):
    em = 0
    for ans in answers:
        ans_ = remove_punctuation(ans)
        prediction_ = remove_punctuation(prediction)
        if ans_ == prediction_:
            em = 1
            break
    return em

# predictions: {example_id: prediction_text}
# references:  {example_id: [answer1, answer2, ...]}
def evaluate_cmrc(predictions, references):
    f1 = 0
    em = 0
    total_count = 0
    skip_count = 0
    for query_id, answers in references.items():
        total_count += 1
        if query_id not in predictions:
            sys.stderr.write('Unanswered question: {}\n'.format(query_id))
            skip_count += 1
            continue
        prediction = predictions[query_id]
        f1 += calc_f1_score(answers, prediction)
        em += calc_em_score(answers, prediction)
    f1_score = 100.0 * f1 / total_count
    em_score = 100.0 * em / total_count
    return {
        'avg': (em_score + f1_score) * 0.5, 
        'f1': f1_score, 
        'em': em_score, 
        'total': total_count, 
        'skip': skip_count
    }

def get_result(start_logits, end_logits, examples, features):

  predicted_answers = {}
  reference_answers = {}

  # 构建example到feature的映射
  example_to_features = collections.defaultdict(list)
  for idx, feature_id in enumerate(features["example_id"]):
    example_to_features[feature_id].append(idx)

  # 指定备选最优答案个数与最大答案长度
  n_best = 20
  max_answer_length = 30

  # 抽取答案
  for example in examples:
    example_id = example["id"]
    context = example["context"]
    answers = []
    # 对当前example对应的所有feature片段进行答案抽取
    for feature_index in example_to_features[example_id]:
      start_logit = start_logits[feature_index]
      end_logit = end_logits[feature_index]
      offsets = features[feature_index]["offset_mapping"]
      start_indexes = np.argsort(start_logit)[:: -1][:n_best].tolist()
      end_indexes = np.argsort(end_logit)[:: -1][:n_best].tolist()
      for start_index in start_indexes:
        for end_index in end_indexes:
          if offsets[start_index] is None or offsets[end_index] is None:
            continue
          if (end_index < start_index or end_index - start_index + 1 > max_answer_length):
            continue
          answers.append(
            {
              "text": context[offsets[start_index][0] : offsets[end_index][1]],
              "logit_score": start_logit[start_index] + end_logit[end_index],
            }
          )
    if len(answers) > 0:
      best_answer = max(answers, key=lambda x: x["logit_score"])
      predicted_answers[example_id] = best_answer["text"]
    else:
      predicted_answers[example_id] = ""
    reference_answers[example_id] = example["answers"]["text"]

  return predicted_answers, reference_answers

def compute_metrics(p):
  start_logits, end_logits = p[0]
  if start_logits.shape[0] == len(tokenized_valid_dataset):
    predicted_answers, reference_answers = get_result(start_logits, end_logits, datasets["validation"], tokenized_valid_dataset)
  else:
    predicted_answers, reference_answers = get_result(start_logits, end_logits, datasets["test"], tokenized_test_dataset)
  return evaluate_cmrc(predicted_answers, reference_answers)


# 训练器配置
model = AutoModelForQuestionAnswering.from_pretrained("hfl/rbt3")
args = TrainingArguments(
  learning_rate=2e-5,
  per_device_train_batch_size=32,
  per_device_eval_batch_size=32,
  num_train_epochs=5,
  weight_decay=0.01,
  output_dir="model_for_qa",
  logging_steps=10,
  evaluation_strategy = "epoch",
  save_strategy = "epoch",
  load_best_model_at_end=True,
  metric_for_best_model="avg",
  fp16=True
)
trainer = Trainer(
  model,
  args,
  train_dataset=tokenized_train_dataset,
  eval_dataset=tokenized_valid_dataset,
  tokenizer=tokenizer,
  compute_metrics=compute_metrics,
  data_collator=default_data_collator,
)


# 训练与评估
trainer.train()
trainer.evaluate(tokenized_test_dataset)
```

进行预测：

```python
start_logits, end_logits = trainer.predict(tokenized_test_dataset).predictions
predicted_answers, reference_answers = get_result(start_logits, end_logits, datasets["test"], tokenized_test_dataset)
print(predicted_answers["TRIAL_800_QUERY_0"])
print(reference_answers["TRIAL_800_QUERY_0"])
```

或者使用pipeline：

```python
from transformers import pipeline

tokenizer = AutoTokenizer.from_pretrained("./model_for_qa/checkpoint-3000")
model = AutoModelForQuestionAnswering.from_pretrained("./model_for_qa/checkpoint-3000")
qa = pipeline("question-answering", model=model, tokenizer=tokenizer)
question = '生命数耗完即算为什么？'
context = '基于《跑跑卡丁车》与《泡泡堂》上所开发的游戏，由韩国Nexon开发与发行。中国大陆由盛大游戏运营，这是Nexon时隔6年再次授予盛大网络其游戏运营权。台湾由游戏橘子运营。玩家以水枪、小枪、锤子或是水炸弹泡封敌人(玩家或NPC)，即为一泡封，将水泡击破为一踢爆。若水泡未在时间内踢爆，则会从水泡中释放或被队友救援(即为一救援)。每次泡封会减少生命数，生命数耗完即算为踢爆。重生者在一定时间内为无敌状态，以踢爆数计分较多者获胜，规则因模式而有差异。以2V2、4V4随机配对的方式，玩家可依胜场数爬牌位(依序为原石、铜牌、银牌、金牌、白金、钻石、大师) ，可选择经典、热血、狙击等模式进行游戏。若游戏中离，则4分钟内不得进行配对(每次中离+4分钟)。开放时间为暑假或寒假期间内不定期开放，8人经典模式随机配对，采计分方式，活动时间内分数越多，终了时可依该名次获得奖励。'
qa(
  question=question,
  context=context,
)
```

