# pipeline

```python
from transformers import pipeline

#this pipline can only generate text one by one
generator = pipeline(
    'text-generation', 
    model="uer/gpt2-chinese-cluecorpussmall",  #可以直接写huggingface上的模型名，也可以写本地的模型地址
    device = 0)

text_inputs = ["客观、严谨、浓缩",
          "地摊文学……",
          "什么鬼玩意，",
          "豆瓣水军果然没骗我。",
          "这是一本社会新闻合集",
          "风格是有点学古龙嘛？但是不好看。"]
import time
start = time.time()
sent_gen = generator(text_inputs, 
            max_length=100, 
            num_return_sequences=2,
            repetition_penalty=1.3, 
            top_k = 20) 
end = time.time()
print("耗时：{}分".format((end - start) / 60))
#返回的sent_gen 形如#[[{'generated_text':"..."},{}],[{},{}]]

for i in sent_gen:
  print(i)

```

耗时：0.15153055588404338分

**缺点：**不能以batch形式生成句子，不能并行，大规模调用的时候时间复杂度较高。

# **TextGenerationPipeline**

```python
from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline

tokenizer = BertTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
model = GPT2LMHeadModel.from_pretrained("uer/gpt2-chinese-cluecorpussmall")

text_generator = TextGenerationPipeline(model, tokenizer, batch_size=3, device=0)
text_inputs = ["客观、严谨、浓缩",
        "地摊文学……",
        "什么鬼玩意，",
        "豆瓣水军果然没骗我。",
        "这是一本社会新闻合集",
        "风格是有点学古龙嘛？但是不好看。"]


import time
start = time.time()
sent_gen = text_generator(text_inputs, 
            max_length=100, 
            num_return_sequences=2,
            repetition_penalty=10.0, 
            num_beams=5,
            do_sample=True,
            top_k = 20) 
end = time.time()
print("耗时：{}分".format((end - start) / 60))
#返回的sent_gen 形如#[[{'generated_text':"..."},{}],[{},{}]]

for sent in sent_gen:
  # gen_seq = sent[0]["generated_text"]
  # print("")
  # print(gen_seq.replace(" ",""))
  print(sent)
```

**优点：**相较方法1，可以设置batch size。

# model.generate()

```python
from transformers import AutoTokenizer, AutoModelWithLMHead
import torch, os

os.environ["CUDA_VISIBLE_DEVICES"] = "2"
tokenizer = AutoTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
model = AutoModelWithLMHead.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
config=model.config

print(config)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = model.to(device)
texts = ["客观、严谨、浓缩",
                "地摊文学……",
                "什么鬼玩意，",
                "豆瓣水军果然没骗我。",
                "这是一本社会新闻合集",
                "风格是有点学古龙嘛？但是不好看。"]
#用batch输入的时候一定要设置padding
encoding = tokenizer(texts, return_tensors='pt', padding=True).to(device)

with torch.no_grad():
    generated_ids = model.generate(**encoding, 
    max_length=200, 
    do_sample=True, #default = False
    top_k=20, #default = 50
    repetition_penalty=3.0 #default = 1.0, use float
    ) 
generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

for l in generated_texts:
    print(l)
```

**缺点：**封装度较差，代码较为冗长。

**优点：**由于是transformers调用模型的通用写法，和其他模型（如bert）的调用方式相似，（如tokenizer的使用），可以举一反三。

# Simple Transformers

**简介：**Simple Transformers基于HuggingFace的Transformers，对特定的NLP经典任务做了高度的封装。在参数的设置上也较为灵活，可以通过词典传入参数。模型的定义和训练过程非常直观，方便理解整个AI模型的流程，很适合NLP新手使用。

simple transformers 指南：[https://simpletransformers.ai/docs/language-generation-model/](https://link.zhihu.com/?target=https%3A//simpletransformers.ai/docs/language-generation-model/)

**优点：**这个包集成了微调的代码，不仅可以直接做生成，进一步微调也非常方便。

**缺点：**有些中文模型不能直接输入huggingface上的模型名称进行自动下载，会报错找不到tokenizer文件，需要手动下载到本地。

# 怎么生成多样的文本

# 前言

最近在做文本生成，用到huggingface transformers库的文本生成 generate() 函数，是 GenerationMixin 类的实现（class transformers.generation_utils.GenerationMixin），是自回归文本生成预训练模型相关参数的集大成者。因此本文解读一下这些参数的含义以及常用的 Greedy Search、Beam Search、Sampling（Temperature、Top-k、Top-p）等各个算法的原理。

这个类对外提供的方法是 generate()，通过调参能完成以下事情：

- greedy decoding：当 num_beams=1 而且 do_sample=False 时，调用 greedy_search()方法，每个step生成条件概率最高的词，因此生成单条文本。
- multinomial sampling：当 num_beams=1 且 do_sample=True 时，调用 sample() 方法，对词表做一个采样，而不是选条件概率最高的词，增加多样性。
- beam-search decoding：当 num_beams>1 且 do_sample=False 时，调用 beam_search() 方法，做一个 num_beams 的柱搜索，每次都是贪婪选择top N个柱。
- beam-search multinomial sampling：当 num_beams>1 且 do_sample=True 时，调用 beam_sample() 方法，相当于每次不再是贪婪选择top N个柱，而是加了一些采样。
- diverse beam-search decoding：当 num_beams>1 且 num_beam_groups>1 时，调用 group_beam_search() 方法。
- constrained beam-search decoding：当 constraints!=None 或者 force_words_ids!=None，实现可控文本生成。

# 各参数的含义

接下来分别看看各个输入参数（[源代码](https://github.com/huggingface/transformers/blob/v4.20.1/src/transformers/generation_utils.py#L844)）：

![在这里插入图片描述](https://img-blog.csdnimg.cn/8609b4617de94074ba66b1c7a2c622ed.png)

我觉得对文本生成质量最有用的几个参数有：max_length、min_length、do_sample、top_k、top_p、repetition_penalty。接下来选择性地记录各个参数的含义。

length_penalty：长度惩罚，默认是1.0。

- length_penalty=1.0：beam search分数会受到生成序列长度的惩罚

- length_penalty=0.0：无惩罚

- length_penalty<0.0：鼓励模型生成长句子

- length_penalty>0.0：鼓励模型生成短句子
	

# 参考

> [【NLP算法】介绍四种常用的模型调用方法|中文文本生成 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/536454472)
>
> [基于 transformers 的 generate() 方法实现多样化文本生成：参数含义和算法原理解读_transformers generate_木尧大兄弟的博客-CSDN博客](https://blog.csdn.net/muyao987/article/details/125917234)

基本上是摘自这两篇文章。