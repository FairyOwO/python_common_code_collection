## 一般操作

直接看代码：

```python
from datasets import load_dataset
from datasets import list_datasets
from pprint import pprint

# 用于展示可用的datasets
pprint(list_datasets())

# 加载datasets
datasets = load_dataset("madao33/new-title-chinese")

# 指定数据
datasets = load_dataset("madao33/new-title-chinese", split="train")
print(datasets)

# 展示数据
datasets = load_dataset("madao33/new-title-chinese")
# 以字典的形式返回数据，键为类型，值为列表
print(datasets["train"][:2])

# 自己划分训练测试数据
datasets = load_dataset("madao33/new-title-chinese")
datasets = datasets["train"]
datasets = datasets.train_test_split(test_size=0.1)
print(datasets)

# 数据的选择和过滤
datasets = load_dataset("madao33/new-title-chinese")
# 通过select根据索引选择
select_datasets = datasets["train"].select([0, 2])
for example in select_datasets:
    # 每一条以字典的形式返回
    print(example)
# 通过filter进行过滤选择
filter_datasets = datasets["train"].filter(lambda example: "中国" in example["title"])
for example in filter_datasets:
    print(example)
    break
    
通过map对每一条数据进行处理
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained(r"C:\Users\Administrator\Desktop\src\ner\model_hub\ner")


def process(example):
    inputs = tokenizer(example["title"],
                       max_length=64,
                       truncation=True,
                       padding="max_length")
    return inputs


processed_datasets = datasets.map(process, batched=True)
print(processed_datasets)
print(processed_datasets["train"][0])

# 保存和加载数据
from datasets import load_from_disk

processed_datasets.save_to_disk("./news_data")
datasets = load_from_disk("./news_data")
print(datasets)
```

加载自定义的数据，可以加载json、csv等格式的数据，也可以zidingpy文件来加载数据，一般情况下，使用json或者csv格式的数据较为简单方便。

json数据的格式：

```json
{
  "version": "v1.0",
  "data": [
    {
      "title": "望海楼美国打“台湾牌”是危险的赌",
      "content": "近期，美国国会众院通过法案，重申美国对台湾的承诺。"
    },
    {
      "title": "望海楼美国打“台湾牌”是危险的赌",
      "content": "近期，美国国会众院通过法案，重申美国对台湾的承诺。"
    }
  ]
}

```

加载数据代码：

```python
# 格式化加载数据
datasets = load_dataset("json", data_files=["data.json"], field="data")
print(datasets)
print(datasets["train"][:2])
```

```
DatasetDict({
    train: Dataset({
        features: ['title', 'content'],
        num_rows: 2
    })
})
{'title': ['望海楼美国打“台湾牌”是危险的赌', '望海楼美国打“台湾牌”是危险的赌'], 'content': ['近期，美国国会众院通过法案，重申美国对台湾的承诺。', '近期，美国国会众院通过法案，重申美国对台湾的承诺。']}
```

查看字段信息：

```python
datasets["train"].features
```

打乱数据并选取部分数据：

```python
datasets = datasets["train"].shuffle(seed=42).select(range(1000))
```

## 处理大数据集

查看datasets占用内存以及本地缓存大小:

```python
!pip install psutil
!pip install zstandard

from datasets import load_dataset

# This takes a few minutes to run, so go grab a tea or coffee while you wait :)
data_files = "https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst"
pubmed_dataset = load_dataset("json", data_files=data_files, split="train")
pubmed_dataset

import psutil

# Process.memory_info is expressed in bytes, so convert to megabytes
print(f"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB")

print(f"Number of files in dataset : {pubmed_dataset.dataset_size}")
size_gb = pubmed_dataset.dataset_size / (1024**3)
print(f"Dataset size (cache file) : {size_gb:.2f} GB")
```

如果你熟悉Pandas，这个结果可能会让你吃惊，因为Wes Kinney的著名经验法则是：你通常需要5到10倍于datasets大小的内存。那么，🤗datasets是如何解决这个内存管理问题的呢？🤗datasets将每个datasets视为内存映射的文件，它提供了RAM和文件系统存储之间的映射，允许库访问和操作datasets的元素，而不需要将其完全加载到内存。

内存映射的文件也可以在多个进程中共享，这使得Dataset.map()等方法可以被并行化，而不需要移动或复制datasets。在引擎盖下，这些功能都是由Apache Arrow内存格式和pyarrow库实现的，这使得数据的加载和处理快如闪电。 关于Apache Arrow的更多细节以及与Pandas的比较，请查看Dejan Simic的博文）。为了看看这一点，让我们在PubMed Abstracts datasets中的所有元素上进行一次小的速度测试：

```python
import timeit

code_snippet = """batch_size = 1000

for idx in range(0, len(pubmed_dataset), batch_size):
    _ = pubmed_dataset[idx:idx + batch_size]
"""

time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())
print(
    f"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in "
    f"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s"
)
```

'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s'

这里我们使用Python的timeit模块来测量code_snippet的执行时间。你通常能够以十分之几到几GB/s的速度迭代一个数据集。这对绝大多数应用来说都是很好的，但有时你不得不处理一个大到甚至无法存储在你的笔记本硬盘上的数据集。例如，如果我们试图下载Pile的全部内容，我们就需要825GB的可用磁盘空间 为了处理这些情况，🤗数据集提供了一个流式功能，允许我们在飞行中下载和访问元素，而不需要下载整个数据集。让我们来看看这个功能是如何工作的。

```python
pubmed_dataset_streamed = load_dataset(
    "json", data_files=data_files, split="train", streaming=True
)
```

我们在本章其他地方遇到的是熟悉的Dataset，而流媒体=True返回的对象是一个IterableDataset。顾名思义，要访问IterableDataset的元素，我们需要对它进行迭代。我们可以按如下方式访问我们的流式数据集的第一个元素：

```python
next(iter(pubmed_dataset_streamed))
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

来自流数据集的元素可以使用IterableDataset.map()进行即时处理，如果你需要对输入进行标记，那么在训练期间是非常有用的。这个过程与我们在第三章中用来标记数据集的过程完全相同，唯一的区别是输出被逐一返回：

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x["text"]))
next(iter(tokenized_dataset))
```

你也可以使用IterableDataset.shuffle()对流式数据集进行洗牌，但与Dataset.shuffle()不同，它只对预定义的缓冲区大小的元素进行洗牌：

```python
shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)
next(iter(shuffled_dataset))
```

在这个例子中，我们从缓冲区的前10,000个例子中随机选择一个例子。一旦一个例子被访问，它在缓冲区中的位置就会被语料库中的下一个例子填满（即上面例子中的第10,001个例子）。你也可以使用IterableDataset.take()和IterableDataset.skip()函数从流数据集中选择元素，这些函数的作用与Dataset.select()类似。例如，为了选择PubMed Abstracts数据集中的前5个例子，我们可以这样做：

```python
dataset_head = pubmed_dataset_streamed.take(5)
list(dataset_head)

[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'pmid': 11409575, 'language': 'eng'},
  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'},
 {'meta': {'pmid': 11409576, 'language': 'eng'},
  'text': "Hypoxaemia in children with severe pneumonia in Papua New Guinea ..."},
 {'meta': {'pmid': 11409577, 'language': 'eng'},
  'text': 'Oxygen concentrators and cylinders ...'},
 {'meta': {'pmid': 11409578, 'language': 'eng'},
  'text': 'Oxygen supply in rural africa: a personal experience ...'}]
```

同样，你可以使用IterableDataset.skip()函数从一个洗过的数据集中创建训练和验证分片，如下所示：

```python
# Skip the first 1,000 examples and include the rest in the training set
train_dataset = shuffled_dataset.skip(1000)
# Take the first 1,000 examples for the validation set
validation_dataset = shuffled_dataset.take(1000)
```

让我们用一个常见的应用来完成我们对数据集流的探索：将多个数据集组合在一起，创建一个单一的语料库。Datasets提供了一个interleave_datasets()函数，可以将一个IterableDataset对象的列表转换成一个IterableDataset，其中新数据集的元素是通过交替使用源实例获得的。当你试图结合大型数据集时，这个函数特别有用，所以作为一个例子，让我们把Pile的FreeLaw子集流出来，这是一个51GB的美国法院的法律意见的数据集：

```python
law_dataset_streamed = load_dataset(
    "json",
    data_files="https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst",
    split="train",
    streaming=True,
)
next(iter(law_dataset_streamed))

{'meta': {'case_ID': '110921.json',
  'case_jurisdiction': 'scotus.tar.gz',
  'date_created': '2010-04-28T17:12:49Z'},
 'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}
```

这个数据集大到足以给大多数笔记本电脑的内存带来压力，但我们却能在不出汗的情况下加载和访问它 现在让我们用interleave_datasets()函数将FreeLaw和PubMed Abstracts数据集的例子结合起来：

```python
from itertools import islice
from datasets import interleave_datasets

combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])
list(islice(combined_dataset, 2))

```

这里我们使用了Python的itertools模块中的islice()函数，从合并的数据集中选择了前两个例子，我们可以看到它们与两个源数据集中的第一个例子相匹配。

最后，如果你想把825GB的Pile全部串联起来，你可以按以下方式抓取所有准备好的文件：

```python
base_url = "https://the-eye.eu/public/AI/pile/"
data_files = {
    "train": [base_url + "train/" + f"{idx:02d}.jsonl.zst" for idx in range(30)],
    "validation": base_url + "val.jsonl.zst",
    "test": base_url + "test.jsonl.zst",
}
pile_dataset = load_dataset("json", data_files=data_files, streaming=True)
next(iter(pile_dataset["train"]))
```

你现在拥有所有你需要的工具来加载和处理各种形状和大小的数据集--但除非你特别幸运，否则在你的NLP旅程中会有一个点，你必须实际创建一个数据集来解决手头的问题。这就是下一节的主题!
