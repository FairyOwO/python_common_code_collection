transformers的`DataCollator`的几个类，都是用于对原始数据集进行前处理，得到做特定任务的数据形式。

| 类名                                       | 继承                            | 用途                                               |
| :----------------------------------------- | :------------------------------ | :------------------------------------------------- |
| DataCollatorMixin                          |                                 | 用于判断处理哪种矩阵torch、tensorflow、numpy       |
| DefaultDataCollator                        | DataCollatorMixin               | 默认Collator,在父类基础上，指定只处理torch矩阵     |
| DataCollatorWithPadding                    |                                 | 仅对输入的batch个tokens，进行padding到指定统一长度 |
| DataCollatorForTokenClassification         | DataCollatorMixin               | 序列标注                                           |
| DataCollatorForSeq2Seq                     |                                 | 翻译、摘要、文本生成                               |
| DataCollatorForLanguageModeling            | DataCollatorMixin               | 语言模型                                           |
| DataCollatorForWholeWordMask               | DataCollatorForLanguageModeling | WWM任务语言模型                                    |
| DataCollatorForSOP                         | DataCollatorForLanguageModeling | 句子顺序预测                                       |
| DataCollatorForPermutationLanguageModeling | DataCollatorMixin               | 乱序语言模型                                       |

## DataCollatorWithPadding

```
( 
tokenizer: PreTrainedTokenizerBase
padding: typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = True
max_length: typing.Optional[int] = None
pad_to_multiple_of: typing.Optional[int] = None
return_tensors: str = 'pt' 
)
```

```python
from pprint import pprint
from transformers import DataCollatorWithPadding, BertTokenizer

model_path = r"C:\Users\Administrator\Desktop\src\ner\model_hub\ner"
tokenizer = BertTokenizer.from_pretrained(model_path)
"""
( tokenizer: PreTrainedTokenizerBasepadding: typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = Truemax_length: typing.Optional[int] = Nonepad_to_multiple_of: typing.Optional[int] = Nonereturn_tensors: str = 'pt' )

tokenizer (PreTrainedTokenizer or PreTrainedTokenizerFast)
padding:
    True or 'longest' (default)
    'max_length': 可以通过指定max_length参数
    'do_not_pad'
return_tensors
"""
dc = DataCollatorWithPadding(tokenizer, max_length=64, padding="max_length")
raw_tokens = tokenizer(['我爱北京', '我爱北京天安门'])
pprint(raw_tokens)

# 将一批数据填充到最大长度
pprint(dc(raw_tokens))
```

## DataCollatorForTokenClassification

```
( 
tokenizer: PreTrainedTokenizerBase
padding: typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = True
max_length: typing.Optional[int] = None
pad_to_multiple_of: typing.Optional[int] = None
label_pad_token_id: int = -100
return_tensors: str = 'pt' )
```

额外需要注意一个参数：Nonelabel_pad_token_id，当标签为-100不计算损失。

```python
from pprint import pprint
from transformers import DataCollatorForTokenClassification, BertTokenizer

model_path = r"C:\Users\Administrator\Desktop\src\ner\model_hub\ner"
t = BertTokenizer.from_pretrained(model_path)

dc = DataCollatorForTokenClassification(t)

features = []
for line, label in [('我爱北京', [-100, 1, 1, 1, 0, -100]), ('我爱北京天安门', [-100, 1, 0, 1, 0, 1, 1, 1, -100])]:
    feature = t(line)
    feature['labels'] = label
    features.append(feature)

pprint(dc(features))
```

需要注意两点：

- 字典中的标签值为labels。
- 标签中如果有类似于bert的[CLS]和[SEP]，标签要对齐。

## DataCollatorForSeq2Seq

参数同上。

```python
from pprint import pprint
from transformers import DataCollatorForSeq2Seq, BertTokenizer

model_path = r"C:\Users\Administrator\Desktop\src\ner\model_hub\ner"
t = BertTokenizer.from_pretrained(model_path)

dc = DataCollatorForSeq2Seq(t)

features = []
for line, label in [('我爱北京', '北京'), ('我爱北京天安门', '天安门')]:
    feature = t(line)
    feature['labels'] = t(label)['input_ids']
    features.append(feature)

pprint(dc(features))
```

需要注意的是文本和标签。

## DataCollatorForLanguageModeling

- 掩词预训练
- 默认有15%的概率选中`input_ids`中的`token`进行掩词,然后在对应`label`特征中给出真实字典id的标签，`label`中其他位置皆为`-100`,即计算交叉熵损失时不考虑。
- 需要注意的是，源代码中，对input_ids特征中被选中掩词的token有三种处理方式
	- 80%被换成`[MASK]`即字典id为`103`
	- 10%被随机换成其他token
	- 10%不做任何处理

```
( 
tokenizer: PreTrainedTokenizerBase
mlm: bool = True
mlm_probability: float = 0.15
pad_to_multiple_of: typing.Optional[int] = None
tf_experimental_compile: bool = False
return_tensors: str = 'pt' 
)
```

```python
from pprint import pprint
from transformers import DataCollatorForLanguageModeling, BertTokenizer

model_path = r"C:\Users\Administrator\Desktop\src\ner\model_hub\ner"
t = BertTokenizer.from_pretrained(model_path)

dc = DataCollatorForLanguageModeling(t)
features = [t('我爱北京'), t('我爱北京天安门')]
pprint(dc(features))
```

## DataCollatorForWholeWordMask

```python
# pip install ltp
from ltp import LTP
ltp = LTP()
print(ltp.seg(["我爱北京天安门,天安门上太阳升"])[0])

# 下载分词预处理脚本
# wget https://raw.githubusercontent.com/huggingface/transformers/v4.10.2/examples/research_projects/mlm_wwm/run_chinese_ref.py

from run_chinese_ref import prepare_ref
from ltp import LTP
ltp = LTP()
ref = prepare_ref(["我爱北京天安门，天安门上太阳升"], ltp, t)
print(ref)

from transformers import BertTokenizer
t = BertTokenizer.from_pretrained("hfl/chinese-roberta-wwm-ext")
feature = t(["我爱北京天安门，天安门上太阳升"])
# 加上子字信息
feature['chinese_ref'] = ref
print(dc(features))
```

DataCollatorForSOP和DataCollatorForPermutationLanguageModeling就不作具体介绍了。