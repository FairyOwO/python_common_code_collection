# 准备

安装包和准备数据集。

```
!pip install peft==0.2.0
!pip install transformers==4.28.1
!pip install accelerate
!pip install loralib
!pip install evaluate
!pip install tqdm
!pip install datasets
!pip install deepspeed
!pip install mpi4py
```

```python
from datasets import load_dataset
dataset_name = "twitter_complaints"
dataset = load_dataset("ought/raft", dataset_name)
classes = [k.replace("_", " ") for k in dataset["train"].features["Label"].names]

dataset = dataset.map(
      lambda x: {"text_label": [classes[label] for label in x["Label"]]},
      batched=True,
      num_proc=1,
  )

print(dataset["train"][2])

"""
{'Tweet text': "If I can't get my 3rd pair of @beatsbydre powerbeats to work today I'm doneski man. This is a slap in my balls. Your next @Bose @BoseService",
 'ID': 2,
 'Label': 1,
 'text_label': 'complaint'}
"""
```

# AutoModelForCausalLM

## BLOOMZ、GPT、OPT、BLOOM

以下是以情感二分类而言，如果是正常的生成任务，处理更简单。

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
model_name_or_path = "bigscience/bloomz-560m"
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
model = AutoModelForCausalLM.from_pretrained(model_name_or_path)

inputs = ["Tweet text : If I can't get my 3rd pair of @beatsbydre powerbeats to work today I'm doneski man. This is a slap in my balls. Your next @Bose @BoseService label : "]
targets = ["complaint"]

# 正常的，将inputs和targets编码成token
model_inputs = tokenizer(inputs)
labels = tokenizer(targets)

"""
'input_ids': [[227985, 5484, 915, 5673, 473, 11229, 2213, 2670, 35307, 28629, 461, 2566, 2765, 1531, 3470, 47134, 10144, 2765, 1531, 427, 2909, 17918, 6782, 27268, 4390, 1517, 17, 3904, 632, 267, 6497, 483, 361, 2670, 101848, 17, 32465, 9585, 2566, 37, 2481, 2566, 37, 2481, 12384, 19248, 915, 210]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}

{'input_ids': [[16449, 5952]], 'attention_mask': [[1, 1]]}
"""

# 针对于训练数据，将文本和标签进行拼接，然后转换。需要注意的是末尾要加上pad_token。在token的最前面进行pad。labels里面只计算标签部分的损失，pad的部分用-100表示。
 import torch
 max_length = 64
 
for i in range(1):
    sample_input_ids = model_inputs["input_ids"][i]
    label_input_ids = labels["input_ids"][i] + [tokenizer.pad_token_id]
    print(label_input_ids)
    # 将文本和标签的ids拼接
    model_inputs["input_ids"][i] = sample_input_ids + label_input_ids
    labels["input_ids"][i] = [-100] * len(sample_input_ids) + label_input_ids
    model_inputs["attention_mask"][i] = [1] * len(model_inputs["input_ids"][i])
 for i in range(1):
    sample_input_ids = model_inputs["input_ids"][i]
    label_input_ids = labels["input_ids"][i]
    # 
    model_inputs["input_ids"][i] = [tokenizer.pad_token_id] * (
        max_length - len(sample_input_ids)
    ) + sample_input_ids
    model_inputs["attention_mask"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[
        "attention_mask"
    ][i]
    labels["input_ids"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids
    model_inputs["input_ids"][i] = torch.tensor(model_inputs["input_ids"][i][:max_length])
    model_inputs["attention_mask"][i] = torch.tensor(model_inputs["attention_mask"][i][:max_length])
    labels["input_ids"][i] = torch.tensor(labels["input_ids"][i][:max_length])

# 对于测试数据，少了abels这一项
for i in range(1):
    sample_input_ids = model_inputs["input_ids"][i]
    model_inputs["input_ids"][i] = [tokenizer.pad_token_id] * (
        max_length - len(sample_input_ids)
    ) + sample_input_ids
    model_inputs["attention_mask"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[
        "attention_mask"
    ][i]
    model_inputs["input_ids"][i] = torch.tensor(model_inputs["input_ids"][i][:max_length])
    model_inputs["attention_mask"][i] = torch.tensor(model_inputs["attention_mask"][i][:max_length])
    
# 最后可以用模型得到输出了
input_ids = torch.tensor([[     3,      3,      3,      3,      3,      3,      3,      3,      3,
             3,      3,      3,      3,      3,      3,      3, 227985,   5484,
           915,   5673,    473,  11229,   2213,   2670,  35307,  28629,    461,
          2566,   2765,   1531,   3470,  47134,  10144,   2765,   1531,    427,
          2909,  17918,   6782,  27268,   4390,   1517,     17,   3904,    632,
           267,   6497,    483,    361,   2670, 101848,     17,  32465,   9585,
          2566,     37,   2481,   2566,     37,   2481,  12384,  19248,    915,
           210]])
attention_mask = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])

labels = torch.tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100, 16449,  5952,     3]])
# 输出分别为loss，logits，past_key_values
output = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
```

除了bloom外，还有opt、gpt英文模型都是在左边进行padding的。

## gpt2-chinese-cluecorpussmall

数据准备：

```python
!wget https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv

from datasets import load_dataset
data_file = "./ChnSentiCorp_htl_all.csv" # 数据文件路径，数据需要提前下载
# 加载数据集
dataset = load_dataset("csv", data_files=data_file)
dataset = dataset.filter(lambda x: x["review"] is not None)
datasets = dataset["train"].train_test_split(0.2, seed=123)
```

使用管道进行预测：

```python
from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline'
model_name_or_path = "uer/gpt2-chinese-cluecorpussmall"
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
model = AutoModelForCausalLM.from_pretrained(model_name_or_path)
text_generator = TextGenerationPipeline(model, tokenizer)   
text_generator("早餐太差，无论去多少人", max_length=100, do_sample=True)
```

使用模型推理进行预测：

```python
# total_predicted_text = "游 泳 池 和 健 身 房 还 不 错 ！ 工"
def select_top_k(predictions, topk=1):
  predicted_index = random.choice(
      predictions[0, -1, :].sort(descending=True)[1][:topk]
  )
  return predicted_index


model = AutoModelForCausalLM.from_pretrained("./gpt2-chinese/")
model.cuda()
model.eval()
examples = dataset["train"]
example = random.choice(examples)
total_predicted_text = example["review"][:10]
print(total_predicted_text)
indexed_tokens = tokenizer.encode(total_predicted_text)[1:-1]
print(indexed_tokens)
tokens_tensor = torch.tensor([indexed_tokens]).cuda()
length = 86
for i in range(length):
  with torch.no_grad():
    outputs = model(tokens_tensor)
    predictions = outputs[0]
    # print(predictions.shape)
    prediction_index = select_top_k(predictions, 10)
    # prediction_index = predictions[0, -1, :].argmax().item()
    # print(prediction_index)
    total_predicted_text += tokenizer.decode(prediction_index)
    if "[PAD]" in total_predicted_text:
      break
    indexed_tokens += [prediction_index]
    tokens_tensor = torch.tensor([indexed_tokens]).cuda()

print(total_predicted_text)
```

