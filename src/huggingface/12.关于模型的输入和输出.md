# 准备

安装包和准备数据集。

```
!pip install peft==0.2.0
!pip install transformers==4.28.1
!pip install accelerate
!pip install loralib
!pip install evaluate
!pip install tqdm
!pip install datasets
!pip install deepspeed
!pip install mpi4py
```

```python
from datasets import load_dataset
dataset_name = "twitter_complaints"
dataset = load_dataset("ought/raft", dataset_name)
classes = [k.replace("_", " ") for k in dataset["train"].features["Label"].names]

dataset = dataset.map(
      lambda x: {"text_label": [classes[label] for label in x["Label"]]},
      batched=True,
      num_proc=1,
  )

print(dataset["train"][2])

"""
{'Tweet text': "If I can't get my 3rd pair of @beatsbydre powerbeats to work today I'm doneski man. This is a slap in my balls. Your next @Bose @BoseService",
 'ID': 2,
 'Label': 1,
 'text_label': 'complaint'}
"""
```

# AutoModelForCausalLM

## BLOOMZ、GPT、OPT、BLOOM

以下是以情感二分类而言，如果是正常的生成任务，处理更简单。

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
model_name_or_path = "bigscience/bloomz-560m"
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
model = AutoModelForCausalLM.from_pretrained(model_name_or_path)

inputs = ["Tweet text : If I can't get my 3rd pair of @beatsbydre powerbeats to work today I'm doneski man. This is a slap in my balls. Your next @Bose @BoseService label : "]
targets = ["complaint"]

# 正常的，将inputs和targets编码成token
model_inputs = tokenizer(inputs)
labels = tokenizer(targets)

"""
'input_ids': [[227985, 5484, 915, 5673, 473, 11229, 2213, 2670, 35307, 28629, 461, 2566, 2765, 1531, 3470, 47134, 10144, 2765, 1531, 427, 2909, 17918, 6782, 27268, 4390, 1517, 17, 3904, 632, 267, 6497, 483, 361, 2670, 101848, 17, 32465, 9585, 2566, 37, 2481, 2566, 37, 2481, 12384, 19248, 915, 210]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}

{'input_ids': [[16449, 5952]], 'attention_mask': [[1, 1]]}
"""

# 针对于训练数据，将文本和标签进行拼接，然后转换。需要注意的是末尾要加上pad_token。在token的最前面进行pad。labels里面只计算标签部分的损失，pad的部分用-100表示。
 import torch
 max_length = 64
 
for i in range(1):
    sample_input_ids = model_inputs["input_ids"][i]
    label_input_ids = labels["input_ids"][i] + [tokenizer.pad_token_id]
    print(label_input_ids)
    # 将文本和标签的ids拼接
    model_inputs["input_ids"][i] = sample_input_ids + label_input_ids
    labels["input_ids"][i] = [-100] * len(sample_input_ids) + label_input_ids
    model_inputs["attention_mask"][i] = [1] * len(model_inputs["input_ids"][i])
 for i in range(1):
    sample_input_ids = model_inputs["input_ids"][i]
    label_input_ids = labels["input_ids"][i]
    # 
    model_inputs["input_ids"][i] = [tokenizer.pad_token_id] * (
        max_length - len(sample_input_ids)
    ) + sample_input_ids
    model_inputs["attention_mask"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[
        "attention_mask"
    ][i]
    labels["input_ids"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids
    model_inputs["input_ids"][i] = torch.tensor(model_inputs["input_ids"][i][:max_length])
    model_inputs["attention_mask"][i] = torch.tensor(model_inputs["attention_mask"][i][:max_length])
    labels["input_ids"][i] = torch.tensor(labels["input_ids"][i][:max_length])

# 对于测试数据，少了abels这一项
for i in range(1):
    sample_input_ids = model_inputs["input_ids"][i]
    model_inputs["input_ids"][i] = [tokenizer.pad_token_id] * (
        max_length - len(sample_input_ids)
    ) + sample_input_ids
    model_inputs["attention_mask"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[
        "attention_mask"
    ][i]
    model_inputs["input_ids"][i] = torch.tensor(model_inputs["input_ids"][i][:max_length])
    model_inputs["attention_mask"][i] = torch.tensor(model_inputs["attention_mask"][i][:max_length])
    
# 最后可以用模型得到输出了
input_ids = torch.tensor([[     3,      3,      3,      3,      3,      3,      3,      3,      3,
             3,      3,      3,      3,      3,      3,      3, 227985,   5484,
           915,   5673,    473,  11229,   2213,   2670,  35307,  28629,    461,
          2566,   2765,   1531,   3470,  47134,  10144,   2765,   1531,    427,
          2909,  17918,   6782,  27268,   4390,   1517,     17,   3904,    632,
           267,   6497,    483,    361,   2670, 101848,     17,  32465,   9585,
          2566,     37,   2481,   2566,     37,   2481,  12384,  19248,    915,
           210]])
attention_mask = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])

labels = torch.tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
         -100, 16449,  5952,     3]])
# 输出分别为loss，logits，past_key_values
output = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
```

除了bloom外，还有opt、gpt英文模型都是在左边进行padding的。

## gpt2-chinese-cluecorpussmall

数据准备：

```python
!wget https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv

from datasets import load_dataset
data_file = "./ChnSentiCorp_htl_all.csv" # 数据文件路径，数据需要提前下载
# 加载数据集
dataset = load_dataset("csv", data_files=data_file)
dataset = dataset.filter(lambda x: x["review"] is not None)
datasets = dataset["train"].train_test_split(0.2, seed=123)
```

使用管道进行预测：

```python
from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline'
model_name_or_path = "uer/gpt2-chinese-cluecorpussmall"
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
model = AutoModelForCausalLM.from_pretrained(model_name_or_path)
text_generator = TextGenerationPipeline(model, tokenizer)   
text_generator("早餐太差，无论去多少人", max_length=100, do_sample=True)
```

使用模型推理进行预测：

```python
# total_predicted_text = "游 泳 池 和 健 身 房 还 不 错 ！ 工"
def select_top_k(predictions, topk=1):
  predicted_index = random.choice(
      predictions[0, -1, :].sort(descending=True)[1][:topk]
  )
  return predicted_index


model = AutoModelForCausalLM.from_pretrained("./gpt2-chinese/")
model.cuda()
model.eval()
examples = dataset["train"]
example = random.choice(examples)
total_predicted_text = example["review"][:10]
print(total_predicted_text)
indexed_tokens = tokenizer.encode(total_predicted_text)[1:-1]
print(indexed_tokens)
tokens_tensor = torch.tensor([indexed_tokens]).cuda()
length = 86
for i in range(length):
  with torch.no_grad():
    outputs = model(tokens_tensor)
    predictions = outputs[0]
    # print(predictions.shape)
    prediction_index = select_top_k(predictions, 10)
    # prediction_index = predictions[0, -1, :].argmax().item()
    # print(prediction_index)
    total_predicted_text += tokenizer.decode(prediction_index)
    if "[PAD]" in total_predicted_text:
      break
    indexed_tokens += [prediction_index]
    tokens_tensor = torch.tensor([indexed_tokens]).cuda()

print(total_predicted_text)
```

# 开源的中文大语言模型

| 项目名                                           | 模型参数大小         | 支持最大长度           |
| ------------------------------------------------ | -------------------- | ---------------------- |
| https://github.com/THUDM/ChatGLM-6B              | ChatGLM-6B           | 2048                   |
| https://github.com/ydli-ai/Chinese-ChatLLaMA     | LLaMA-7B/13B         | 2048                   |
| https://github.com/LianjiaTech/BELLE             | BLOOMZ-7B            | 2048                   |
| https://github.com/PhoebusSi/Alpaca-CoTChat      | ChatGLM-6B-CoT\|Moss | 支持多种中文模型的微调 |
| https://github.com/ymcui/Chinese-LLaMA-Alpaca    | LLaMA-7B/13B         | 2048                   |
| https://github.com/LC1332/Luotuo-Chinese-LLM     | LLaMA-7B             | 2048                   |
| https://github.com/LC1332/CamelBell-Chinese-LoRA | ChatGLM-6B           | 2048                   |
| https://github.com/Facico/Chinese-Vicuna         | LLaMA-7B/13B         | 2048                   |
| https://github.com/OpenLMLab/MOSS                | moss-monn-16B        | 2048                   |
| https://github.com/CVI-SZU/Linly                 | LLaMA-7B/13B         | 2048                   |
| https://github.com/FreedomIntelligence/LLMZoo    | LLaMA-7B/13B         | 2048                   |
| https://github.com/BlinkDL/ChatRWKV              | RWKV-1.5B/3B/7B/14B  | 1024                   |

这里简单说明下不同的模型：

- ChatGLM-6B：清华大学开源的基于GLM模型而成的类似于ChatGPT的模型。
- LLaMA（羊驼）：Meta开源的大模型 LLaMa 。
- Alpaca：斯坦福基于LLaMA进行指令微调而得到的。
- moss：基于CodeGen开发的类似ChatGPT的模型。
- RWKV：一种类似RNN的Transformer。
- BLOOMZ：基于Bloom进行指令微调而得到的多语言模型。

## ChatGLM-6B的输入和输出

最基本的使用：

```python
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b-int4", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm-6b-int4", trust_remote_code=True).half().cuda()
model = model.eval()
response, history = model.chat(tokenizer, "你好", history=[])
print(response)
```

具体以数据看看模型的输入和输出：

```python
class Args:
  max_source_length = 64
  max_target_length = 64
  ignore_pad_token_for_loss = True

data_args = Args()
example = {
    "input": ["类型#上衣*材质#牛仔布*颜色#白色*风格#简约*图案#刺绣*衣样式#外套*衣款式#破洞"],
    "label": ["简约而不简单的牛仔外套,白色的衣身十分百搭。衣身多处有做旧破洞设计,打破单调乏味,增加一丝造型看点。衣身后背处有趣味刺绣装饰,丰富层次感,彰显别样时尚。"]
}

prompt_column = "input"
response_column = "label"
history_column = None
prefix = ""

"""
"bos_token": "<sop>",
"eos_token": "<eop>",
"end_token": "</s>",
"gmask_token": "[gMASK]",
"mask_token": "[MASK]",
"pad_token": "<pad>",
"unk_token": "<unk>",
"""

def preprocess_function_train(examples):
  max_seq_length = data_args.max_source_length + data_args.max_target_length

  model_inputs = {
      "input_ids": [],
      "labels": [],
  }
  for i in range(len(examples[prompt_column])):
      if examples[prompt_column][i] and examples[response_column][i]:
          query, answer = examples[prompt_column][i], examples[response_column][i]

          if history_column is None:
              prompt = query
          else:
              prompt = ""
              history = examples[history_column][i]
              for turn_idx, (old_query, response) in enumerate(history):
                  prompt += "[Round {}]\n问：{}\n答：{}\n".format(turn_idx, old_query, response)
              prompt += "[Round {}]\n问：{}\n答：".format(len(history), query)

          prompt = prefix + prompt
          a_ids = tokenizer.encode(text=prompt, add_special_tokens=False)
          b_ids = tokenizer.encode(text=answer, add_special_tokens=False)

          print_dataset_example(a_ids, b_ids)
          
          if len(a_ids) > data_args.max_source_length - 1:
              a_ids = a_ids[: data_args.max_source_length - 1]

          if len(b_ids) > data_args.max_target_length - 2:
              b_ids = b_ids[: data_args.max_target_length - 2]

          input_ids = tokenizer.build_inputs_with_special_tokens(a_ids, b_ids)

          print(input_ids)
          print(tokenizer.convert_ids_to_tokens(input_ids))
          print(tokenizer.decode(input_ids))

          context_length = input_ids.index(tokenizer.bos_token_id) # sop
          mask_position = context_length - 1
          labels = [-100] * context_length + input_ids[mask_position+1:]
          print(labels)
          
          pad_len = max_seq_length - len(input_ids)
          input_ids = input_ids + [tokenizer.pad_token_id] * pad_len
          labels = labels + [tokenizer.pad_token_id] * pad_len
          if data_args.ignore_pad_token_for_loss:
              labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]

          model_inputs["input_ids"].append(input_ids)
          model_inputs["labels"].append(labels)

  return model_inputs

def print_dataset_example(input_input_ids, label_input_ids):
    print("input_ids",input_input_ids)
    print("input_tokens", tokenizer.convert_ids_to_tokens(input_input_ids))
    print("inputs", tokenizer.decode(input_input_ids))
    print("label_ids", label_input_ids)
    print("label_tokens", tokenizer.convert_ids_to_tokens(label_input_ids))
    print("labels", tokenizer.decode(label_input_ids))


res = preprocess_function_train(example)

"""
input_ids [5, 65421, 61, 75898, 32, 68554, 61, 77257, 64555, 32, 65107, 61, 66268, 32, 65347, 61, 71689, 32, 69768, 61, 85428, 32, 65173, 73942, 61, 70984, 32, 65173, 70936, 61, 64703, 65509]
input_tokens ['▁', '类型', '#', '上衣', '*', '材质', '#', '牛仔', '布', '*', '颜色', '#', '白色', '*', '风格', '#', '简约', '*', '图案', '#', '刺绣', '*', '衣', '样式', '#', '外套', '*', '衣', '款式', '#', '破', '洞']
inputs 类型#上衣*材质#牛仔布*颜色#白色*风格#简约*图案#刺绣*衣样式#外套*衣款式#破洞
label_ids [5, 71689, 66561, 67061, 77257, 70984, 6, 72194, 65173, 64290, 64622, 81549, 63823, 65173, 64290, 83343, 63832, 63912, 65209, 64703, 65509, 64051, 6, 69418, 78598, 87019, 6, 64257, 71319, 66069, 74197, 63823, 65173, 72265, 64880, 64131, 63832, 73416, 85428, 66261, 6, 65594, 87834, 6, 73412, 105145, 65388, 63823]
label_tokens ['▁', '简约', '而不', '简单的', '牛仔', '外套', ',', '白色的', '衣', '身', '十分', '百搭', '。', '衣', '身', '多处', '有', '做', '旧', '破', '洞', '设计', ',', '打破', '单调', '乏味', ',', '增加', '一丝', '造型', '看点', '。', '衣', '身后', '背', '处', '有', '趣味', '刺绣', '装饰', ',', '丰富', '层次感', ',', '彰显', '别样', '时尚', '。']
labels 简约而不简单的牛仔外套,白色的衣身十分百搭。衣身多处有做旧破洞设计,打破单调乏味,增加一丝造型看点。衣身后背处有趣味刺绣装饰,丰富层次感,彰显别样时尚。
[5, 65421, 61, 75898, 32, 68554, 61, 77257, 64555, 32, 65107, 61, 66268, 32, 65347, 61, 71689, 32, 69768, 61, 85428, 32, 65173, 73942, 61, 70984, 32, 65173, 70936, 61, 64703, 65509, 130001, 130004, 5, 71689, 66561, 67061, 77257, 70984, 6, 72194, 65173, 64290, 64622, 81549, 63823, 65173, 64290, 83343, 63832, 63912, 65209, 64703, 65509, 64051, 6, 69418, 78598, 87019, 6, 64257, 71319, 66069, 74197, 63823, 65173, 72265, 64880, 64131, 63832, 73416, 85428, 66261, 6, 65594, 87834, 6, 73412, 105145, 65388, 63823, 130005]
['▁', '类型', '#', '上衣', '*', '材质', '#', '牛仔', '布', '*', '颜色', '#', '白色', '*', '风格', '#', '简约', '*', '图案', '#', '刺绣', '*', '衣', '样式', '#', '外套', '*', '衣', '款式', '#', '破', '洞', '[gMASK]', '<sop>', '▁', '简约', '而不', '简单的', '牛仔', '外套', ',', '白色的', '衣', '身', '十分', '百搭', '。', '衣', '身', '多处', '有', '做', '旧', '破', '洞', '设计', ',', '打破', '单调', '乏味', ',', '增加', '一丝', '造型', '看点', '。', '衣', '身后', '背', '处', '有', '趣味', '刺绣', '装饰', ',', '丰富', '层次感', ',', '彰显', '别样', '时尚', '。', '<eop>']
类型#上衣*材质#牛仔布*颜色#白色*风格#简约*图案#刺绣*衣样式#外套*衣款式#破洞 简约而不简单的牛仔外套,白色的衣身十分百搭。衣身多处有做旧破洞设计,打破单调乏味,增加一丝造型看点。衣身后背处有趣味刺绣装饰,丰富层次感,彰显别样时尚。
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 71689, 66561, 67061, 77257, 70984, 6, 72194, 65173, 64290, 64622, 81549, 63823, 65173, 64290, 83343, 63832, 63912, 65209, 64703, 65509, 64051, 6, 69418, 78598, 87019, 6, 64257, 71319, 66069, 74197, 63823, 65173, 72265, 64880, 64131, 63832, 73416, 85428, 66261, 6, 65594, 87834, 6, 73412, 105145, 65388, 63823, 130005]
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 71689, 66561, 67061, 77257, 70984, 6, 72194, 65173, 64290, 64622, 81549, 63823, 65173, 64290, 83343, 63832, 63912, 65209, 64703, 65509, 64051, 6, 69418, 78598, 87019, 6, 64257, 71319, 66069, 74197, 63823, 65173, 72265, 64880, 64131, 63832, 73416, 85428, 66261, 6, 65594, 87834, 6, 73412, 105145, 65388, 63823, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
"""
```

模型还是使用AutoModel.from_pretrained，训练器使用的是transformers自带的Seq2SeqTrainer。

## Chinese-LLaMA的输入和输出

### 依赖

```python
pip install torch==1.13.1
pip install transformers
pip install sentencepiece
pip install git+https://github.com/huggingface/peft
```

```python
import torch
import json
from torch.utils.data import Dataset

def load_data(path):
    with open(path, "r") as fp:
        data = fp.read().strip().split("\n") 
    return data


def print_dataset_example(input_input_ids, label_input_ids, tokenizer):
    print("input_ids",input_input_ids)
    print("input_tokens", tokenizer.convert_ids_to_tokens(input_input_ids))
    print("inputs", tokenizer.decode(input_input_ids))
    print("label_ids", label_input_ids)
    print("label_tokens", tokenizer.convert_ids_to_tokens(label_input_ids))
    print("labels", tokenizer.decode(label_input_ids))

PROMPT_TEMPLATE = (
        "Below is an instruction that describes a task. "
        "Write a response that appropriately completes the request.\n\n"
        "### Instruction:\n{instruction}\n\n### Response: "
    )

IGNORE_INDEX = -100
    
class NerCollate:
    def __init__(self, args, tokenizer):
        self.instruct_column = args.instruct_column
        self.query_column = args.query_column
        self.response_column = args.response_column
        self.history_column = None
        self.tokenizer = tokenizer
        self.max_seq_length = args.max_seq_length
        
    def collate_fn(self, batch):
        sources = []
        targets = []
        prompt = PROMPT_TEMPLATE
        for example in batch:
            if isinstance(example, str):
                example = json.loads(example)
            instruction = example[self.instruct_column]
            input = example[self.query_column]
            output = example[self.response_column]
            if input is not None and input !="":
                instruction = instruction+'\n'+input
            source = prompt.format_map({'instruction':instruction})
            target = f"{output}{self.tokenizer.eos_token}"

            # print(json.dumps(source, ensure_ascii=False), json.dumps(target, ensure_ascii=False))
            sources.append(source)
            targets.append(target)

        tokenized_sources = self.tokenizer(sources,return_attention_mask=False)
        tokenized_targets = self.tokenizer(targets,return_attention_mask=False,add_special_tokens=False)


        # print(tokenized_sources)
        # print(tokenized_targets)
        all_input_ids = []
        all_labels = []
        for s,t in zip(tokenized_sources['input_ids'],tokenized_targets['input_ids']):
            input_ids = (s + t)[:self.max_seq_length]
            labels = ([IGNORE_INDEX] * len(s) + t)[:self.max_seq_length]
            assert len(input_ids) == len(labels)
            input_ids = input_ids + [self.tokenizer.pad_token_id] * (self.max_seq_length - len(input_ids))
            labels = labels + [IGNORE_INDEX] * (self.max_seq_length - len(labels))
            # print(input_ids)
            # print(labels)
            all_input_ids.append(input_ids)
            all_labels.append(labels)

            # print(tokenizer.decode(input_ids))
            # print(labels)
        results = {'input_ids': torch.tensor(all_input_ids), 'labels': torch.tensor(all_labels)}
        return results
    
if __name__ == "__main__":
  class Args:
    max_seq_length = 128+64
    instruct_column = "instruct"
    query_column = "query"
    response_column = "answer"
    train_path = "data/msra/instruct_data/train.txt"

  args = Args()
  from transformers import LlamaTokenizer
  tokenizer = LlamaTokenizer.from_pretrained("./model_hub/chinese-alpaca-7b", trust_remote_code=True)
  data = load_data(args.train_path)[:10]
  print(data[0])

  ner_collate = NerCollate(args, tokenizer)
  
  from torch.utils.data import DataLoader
  train_dataloader = DataLoader(data,
                  batch_size=2,
                  shuffle=False,
                  drop_last=True,
                  num_workers=0,
                  collate_fn=ner_collate.collate_fn)
  for step, batch in enumerate(train_dataloader):
    input_ids = batch["input_ids"]
    labels = batch["labels"]
    print(input_ids.shape, labels.shape)
    break

  # train_dataset = ner_collate.collate_fn(data) 
  # print(train_dataset["input_ids"][0])
```









# 参考

>[butyuhao/Awesome-Chinese-LLM (github.com)](https://github.com/butyuhao/Awesome-Chinese-LLM)
