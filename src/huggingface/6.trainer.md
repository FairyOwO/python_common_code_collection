## 简介

Trainer模块是基础组件的最后一个模块，它封装了一套完整的在数据集上训练、评估与预测的流程。

借助Trainer模块，可以快速启动训练。

Trainer模块主要包含两部分的内容：TrainingArguments与Trainer，前者用于训练参数的设置，后者用于创建真正的训练器，进行训练、评估预测等实际操作。

此外，针对Seq2Seq训练任务，提供了专门的Seq2SeqTrainingArguments与Seq2SeqTrainer，整体与TrainingArguments和Trainer类似，但是提供了专门用于生成的部分参数。

## TrainingArguments

TrainingArguments中可以配置整个训练过程中使用的参数，默认版本是包含90个参数，涉及模型存储、模型优化、训练日志、GPU使用、模型精度、分布式训练等多方面的配置内容，这里就不一一介绍了，

![img](https://pic1.zhimg.com/v2-7cb0cfe598e04cac8d348e8caf657624_r.jpg)

Seq2SeqTrainingArguments中除了上述的内容还包括生成部分的参数设置，如是否要进行生成、最大长度等共94个参数。

## Trainer

Trainer中配置具体的训练用到的内容，包括模型、训练参数、训练集、验证集、分词器、评估函数等内容。

当指定完上述对应参数，便可以通过调用train方法进行模型训练；训练完成后可以通过调用evaluate方法对模型进行评估；得到满意的模型后，最后调用predict方法对数据集进行预测。

```python
from transformers import TrainingArguments, Trainer
# 创建TrainingArguments
training_args = TrainingArguments(...)
# 创建Trainer
trainer = Trainer(..., args=training_args, ...)
# 模型训练
trainer.train()
# 模型评估
trainer.evaluate()
# 模型预测
trainer.predict()
```

不过，如果在创建Trainer对象时没有指定评估函数，那么调用evaluate方法时只能展示loss的信息。

**需要特别注意的是，使用Trainer进行模型训练对模型的输入输出是有限制的，要求模型返回ModelOutput的元组或子类，同时如果提供了标签，模型要能返回loss结果，并且loss要作为ModelOutput元组的第一个值**。

![image-20230509175119276](C:\Users\Administrator\Desktop\github\python_common_code_collection\src\huggingface\6.trainer.assets\image-20230509175119276.png)

也就是说我们可以自定义Trainer，继承Trainer后重写上述的一些方法。

我们可以自定义损失计算：

```python
from torch import nn
from transformers import Trainer


class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        # forward pass
        outputs = model(**inputs)
        logits = outputs.get("logits")
        # compute custom loss (suppose one has 3 labels with different weights)
        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0]))
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss
```

一些参数：

- **model** ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/model#transformers.PreTrainedModel) or `torch.nn.Module`, *optional*) — The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.[Trainer](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer) is optimized to work with the [PreTrainedModel](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/model#transformers.PreTrainedModel) provided by the library. You can still use your own models defined as `torch.nn.Module` as long as they work the same way as the 🤗 Transformers models.
- **args** ([TrainingArguments](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.TrainingArguments), *optional*) — The arguments to tweak for training. Will default to a basic instance of [TrainingArguments](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.TrainingArguments) with the `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.
- **data_collator** (`DataCollator`, *optional*) — The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will default to [default_data_collator()](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/data_collator#transformers.default_data_collator) if no `tokenizer` is provided, an instance of [DataCollatorWithPadding](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/data_collator#transformers.DataCollatorWithPadding) otherwise.
- **train_dataset** (`torch.utils.data.Dataset` or `torch.utils.data.IterableDataset`, *optional*) — The dataset to use for training. If it is a [Dataset](https://huggingface.co/docs/datasets/v2.11.0/en/package_reference/main_classes#datasets.Dataset), columns not accepted by the `model.forward()` method are automatically removed.Note that if it’s a `torch.utils.data.IterableDataset` with some randomization and you are training in a distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally sets the seed of the RNGs used.
- **eval_dataset** (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`]), *optional*) — The dataset to use for evaluation. If it is a [Dataset](https://huggingface.co/docs/datasets/v2.11.0/en/package_reference/main_classes#datasets.Dataset), columns not accepted by the `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each dataset prepending the dictionary key to the metric name.
- **tokenizer** ([PreTrainedTokenizerBase](https://huggingface.co/docs/transformers/v4.28.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase), *optional*) — The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs to the maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an interrupted training or reuse the fine-tuned model.
- **model_init** (`Callable[[], PreTrainedModel]`, *optional*) — A function that instantiates the model to be used. If provided, each call to [train()](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer.train) will start from a new instance of the model as given by this function.The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to be able to choose different architectures according to hyper parameters (such as layer count, sizes of inner layers, dropout probabilities etc).
- **compute_metrics** (`Callable[[EvalPrediction], Dict]`, *optional*) — The function that will be used to compute metrics at evaluation. Must take a [EvalPrediction](https://huggingface.co/docs/transformers/v4.28.1/en/internal/trainer_utils#transformers.EvalPrediction) and return a dictionary string to metric values.
- **callbacks** (List of [TrainerCallback](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/callback#transformers.TrainerCallback), *optional*) — A list of callbacks to customize the training loop. Will add those to the list of default callbacks detailed in [here](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/callback).If you want to remove one of the default callbacks used, use the [Trainer.remove_callback()](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer.remove_callback) method.
- **optimizers** (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*) — A tuple containing the optimizer and the scheduler to use. Will default to an instance of [AdamW](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/optimizer_schedules#transformers.AdamW) on your model and a scheduler given by [get_linear_schedule_with_warmup()](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/optimizer_schedules#transformers.get_linear_schedule_with_warmup) controlled by `args`.
- **preprocess_logits_for_metrics** (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*) — A function that preprocess the logits right before caching them at each evaluation step. Must take two tensors, the logits and the labels, and return the logits once processed as desired. The modifications made by this function will be reflected in the predictions received by `compute_metrics`.

重要的一些属性：

Important attributes:

- **model** — Always points to the core model. If using a transformers model, it will be a [PreTrainedModel](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/model#transformers.PreTrainedModel) subclass.
- **model_wrapped** — Always points to the most external model in case one or more other modules wrap the original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`, the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner model hasn’t been wrapped, then `self.model_wrapped` is the same as `self.model`.
- **is_model_parallel** — Whether or not a model has been switched to a model parallel mode (different from data parallelism, this means some of the model layers are split on different GPUs).
- **place_model_on_device** — Whether or not to automatically place the model on the device - it will be set to `False` if model parallel or deepspeed is used, or if the default `TrainingArguments.place_model_on_device` is overridden to return `False` .
- **is_in_train** — Whether or not a model is currently running `train` (e.g. when `evaluate` is called while in `train`)

也就是说除了数据并行之外，还可以进行模型并行。

更多请参考：[Trainer (huggingface.co)](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#trainer)