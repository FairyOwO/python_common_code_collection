```
!pip install transformers
!pip install datasets
!pip install seqeval
!pip install evaluate
!pip install accelerate
!pip install sentencepiece
!pip install sacrebleu
!pip install sacremoses
```

```python
from datasets import load_dataset

raw_datasets = load_dataset("opus100", language_pair="en-zh")

# { "en": "Not since the inception of its mandate in 1968 has the Committee ever confronted such anger and misery among the Palestinian people and other Arabs in occupied territories and disrespect for their basic human rights and fundamental freedoms.", "zh": "大概自1968年委员会开始执行其任务以来，委员会从未遇到被占领土内的巴勒斯坦人和其他阿拉伯人如此愤怒和悲惨过，以及其基本人权和基本自由如此不受尊重。" }

"""
from transformers import AutoTokenizer

model_checkpoint = "Helsinki-NLP/opus-mt-en-zh"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors="pt")
split_datasets = raw_datasets
en_sentence = split_datasets["train"][1]["translation"]["en"]
zh_sentence = split_datasets["train"][1]["translation"]["zh"]

inputs = tokenizer(en_sentence, text_target=zh_sentence)
#如果你忘了说明你正在对标签进行标记化，它们将被输入标记器标记化
wrong_targets = tokenizer(zh_sentence)
print(tokenizer.convert_ids_to_tokens(wrong_targets["input_ids"]))
print(tokenizer.convert_ids_to_tokens(inputs["labels"]))

['▁', '<unk>', '...', '▁', '<unk>', '▁', '<unk>', '</s>']
['▁', '减轻', '酸', '...', '▁', '酸', '痛', '的', '药', '▁', '减轻', '酸', '痛', '的', '药', '</s>']
"""

max_length = 128


def preprocess_function(examples):
    inputs = [ex["en"] for ex in examples["translation"]]
    targets = [ex["zh"] for ex in examples["translation"]]
    model_inputs = tokenizer(
        inputs, text_target=targets, max_length=max_length, truncation=True
    )
    return model_inputs
split_datasets["train"] = split_datasets["train"].select(range(2000))
tokenized_datasets = split_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=split_datasets["train"].column_names,
)

from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)


import numpy as np


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    # In case the model returns more than the prediction logits
    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # Replace -100s in the labels as we can't decode them
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    return {"bleu": result["score"]}

from transformers import Seq2SeqTrainingArguments

args = Seq2SeqTrainingArguments(
    f"marian-finetuned-kde4-en-to-zh",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=False,
)
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
trainer.train()
trainer.evaluate(max_length=max_length)
"""
{'eval_loss': 1.7467373609542847,
 'eval_bleu': 22.504079006165274,
 'eval_runtime': 143.9334,
 'eval_samples_per_second': 13.895,
 'eval_steps_per_second': 0.222,
 'epoch': 3.0}
"""
```

预测：

```python
from transformers import pipeline
import random

i = random.choice(range(100))

example = raw_datasets["train"][i]["translation"]
print(example)
from transformers import pipeline

model_checkpoint = "Helsinki-NLP/opus-mt-en-zh"
translator = pipeline("translation", model=model_checkpoint)
print(translator(example["en"]))

model_checkpoint = "marian-finetuned-kde4-en-to-zh/checkpoint-189"
translator = pipeline("translation", model=model_checkpoint)
print(translator(example["en"]))

"""
{'en': 'Grandma!', 'zh': '奶奶 ！'}
[{'translation_text': '外婆! 外婆! 外婆! 外婆! 外婆! 外婆!'}]
[{'translation_text': '奶奶!'}]
"""
```

以下是使用自定义的训练：

```python
from torch.utils.data import DataLoader
from datasets import load_dataset

raw_datasets = load_dataset("opus100", language_pair="en-zh")
split_datasets = raw_datasets

max_length = 128


def preprocess_function(examples):
    inputs = [ex["en"] for ex in examples["translation"]]
    targets = [ex["zh"] for ex in examples["translation"]]
    model_inputs = tokenizer(
        inputs, text_target=targets, max_length=max_length, truncation=True
    )
    return model_inputs
split_datasets["train"] = split_datasets["train"].select(range(2000))
split_datasets["validation"] = split_datasets["validation"].select(range(200))
tokenized_datasets = split_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=split_datasets["train"].column_names,
)

tokenized_datasets.set_format("torch")

train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

def postprocess(predictions, labels):
    predictions = predictions.cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]
    return decoded_preds, decoded_labels

from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

output_dir = "marian"

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=128,
            )
        labels = batch["labels"]

        # Necessary to pad predictions and labels for being gathered
        generated_tokens = accelerator.pad_across_processes(
            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
        )
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(generated_tokens)
        labels_gathered = accelerator.gather(labels)

        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=decoded_preds, references=decoded_labels)

    results = metric.compute()
    print(f"epoch {epoch}, BLEU score: {results['score']:.2f}")

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
```

预测：

```python
from transformers import pipeline
import random

i = random.choice(range(100))

example = raw_datasets["train"][i]["translation"]
print(example)
from transformers import pipeline

model_checkpoint = "Helsinki-NLP/opus-mt-en-zh"
translator = pipeline("translation", model=model_checkpoint)
print(translator(example["en"]))

model_checkpoint = "marian"
translator = pipeline("translation", model=model_checkpoint)
print(translator(example["en"]))
```

